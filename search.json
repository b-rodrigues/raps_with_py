[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Building reproducible analytical pipelines with Python",
    "section": "",
    "text": "Welcome!"
  },
  {
    "objectID": "index.html#how-using-a-few-ideas-from-software-engineering-can-help-data-scientists-analysts-and-researchers-write-reliable-code",
    "href": "index.html#how-using-a-few-ideas-from-software-engineering-can-help-data-scientists-analysts-and-researchers-write-reliable-code",
    "title": "Building reproducible analytical pipelines with Python",
    "section": "How using a few ideas from software engineering can help data scientists, analysts and researchers write reliable code",
    "text": "How using a few ideas from software engineering can help data scientists, analysts and researchers write reliable code\nThis is the Python edition of the book “Building reproducible analytical pipelines”, and it’s a work-in-progress. If you’re looking for the R version, visit this link.\n\n\n \n\nData scientists, statisticians, analysts, researchers, and many other professionals write a lot of code.\nNot only do they write a lot of code, but they must also read and review a lot of code as well. They either work in teams and need to review each other’s code, or need to be able to reproduce results from past projects, be it for peer review or auditing purposes. And yet, they never, or very rarely, get taught the tools and techniques that would make the process of writing, collaborating, reviewing and reproducing projects possible.\nWhich is truly unfortunate because software engineers face the same challenges and solved them decades ago.\nThe aim of this book is to teach you how to use some of the best practices from software engineering and DevOps to make your projects robust, reliable and reproducible. It doesn’t matter if you work alone, in a small or in a big team. It doesn’t matter if your work gets (peer-)reviewed or audited: the techniques presented in this book will make your projects more reliable and save you a lot of frustration!\nAs someone whose primary job is analysing data, you might think that you are not a developer. It seems as if developers are these genius types that write extremely high-quality code and create these super useful packages. The truth is that you are a developer as well. It’s just that your focus is on writing code for your purposes to get your analyses going instead of writing code for others. Or at least, that’s what you think. Because in others, your team-mates are included. Reviewers and auditors are included. Any people that will read your code are included, and there will be people that will read your code. At the very least future you will read your code. By learning how to set up projects and write code in a way that future you will understand and not want to murder you, you will actually work towards improving the quality of your work, naturally.\nThe book can be read for free on https://b-rodrigues.github.io/raps_with_py/ and you’ll be able buy a DRM-free Epub or PDF on Leanpub1 once there’s more content.\nThis is the Python edition of my book titled Building reproducible analytical pipelines with R2. This means that a lot of text is copied over, but the all of the code and concepts are completely adapted to the Python programming language. This book is also shorter than the R version. Here’s the topics that I will cover:\n\nDependency management with pipenv;\nSome thoughts on functional programming with Python;\nUnit and assertive testing;\nBuild automation with ploomber;\nLiterate programming with Quarto;\nReproducible environments with Docker;\nContinuous integration and delivery.\n\nWhile this is not a book for beginners (you really should be familiar with Python before reading this), I will not assume that you have any knowledge of the tools discussed. But be warned, this book will require you to take the time to read it, and then type on your computer. Type a lot.\nI hope that you will enjoy reading this book and applying the ideas in your day-to-day, ideas which hopefully should improve the reliability, traceability and reproducibility of your code. You can read this book for free on TO UPDATE\nYou can also buy a physical copy of the book on Amazon.\nIf you want to get to know me better, read my bio3.\nYou’ll also be able to buy a physical copy of the book on Amazon once it’s done. In the meantime, you could buy the R edition.\nIf you find this book useful, don’t hesitate to let me know or leave a rating on Amazon!\nYou can submit issues, PRs and ask questions on the book’s Github repository4."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Building reproducible analytical pipelines with Python",
    "section": "",
    "text": "https://leanpub.com/↩︎\nhttps://raps-with-r.dev↩︎\nhttps://www.brodrigues.co/about/me/↩︎\nhttps://github.com/b-rodrigues/raps_with_py↩︎"
  },
  {
    "objectID": "preface.html#footnotes",
    "href": "preface.html#footnotes",
    "title": "Preface",
    "section": "",
    "text": "www.raps-with-r.dev↩︎\nhttps://ukgovdatascience.github.io/rap_companion/↩︎\nhttps://software-carpentry.org/↩︎"
  },
  {
    "objectID": "setting_up.html#why-is-installing-python-such-a-hard-problem",
    "href": "setting_up.html#why-is-installing-python-such-a-hard-problem",
    "title": "1  Setting up a development environment",
    "section": "1.1 Why is installing Python such a hard problem?",
    "text": "1.1 Why is installing Python such a hard problem?\nIf you google “how to install Python” you will find a surprising amount of articles explaining how to do it. I say “surprising amount” because one might expect to install Python like any other piece of software. If you’re already familiar with R, you could think that installing Python would be done the same way: download the installer for your operating system, and then install it. And, actually, you can do just that for Python as well. So why are there 100s of articles online explaining how to install Python, and why aren’t all of these articles simply telling you to download the installer to install Python? Why did I write this chapter on installing Python?\nWell, there are several thing that we need to deal with if we want to install and use Python the “right way”. First of all, Python is pre-installed on Linux distributions and older versions of macOS. So if you’re using one of these operating systems, you could use the built-in Python interpreter, but this is not recommended. The reason being that these bundled versions are generally older, and that you don’t control their upgrade process, as these get updated alongside the operating system. On Windows and newer versions of macOS, Python is, as far as I know, never bundled, so you’d need to install it anyways.\nAnother thing that you need to consider is that newer Python versions can introduce breaking changes, making code written for an earlier version of Python not run on a newer version of Python. This is not a Python-specific issue: it happens with any programming language. So this means that ideally you would want to bundle a Python version with your project’s code.\nThe same holds true for individual packages: newer versions of packages might not even work with older releases of Python, so to avoid any issues, an analysis would get bundled with a Python release and Python packages. This bundle is what I call a development environment, and in order to build such development environment, specific tools have to be used. And there’s a lot of these tools in the Python ecosystem… so much so that when you’re first starting, you might get lost. So here is he two tools that I use for this, and that I think work quite well: pyenv and pipenv."
  },
  {
    "objectID": "setting_up.html#first-step-installing-python",
    "href": "setting_up.html#first-step-installing-python",
    "title": "1  Setting up a development environment",
    "section": "1.2 First step: installing Python",
    "text": "1.2 First step: installing Python\nIn this section, I will assume that you have no version of Python already installed on your computer. If you do have Python, you may want to skip this section: but know that if you’re not using pyenv to download and install Python versions, you will need to keep installing your environments manually. pipenv however install environments automatically if pyenv is available, so you might want to switch over to pyenv.\nSo first of all, let’s install pyenv. pyenv is a tool that allows you to install as many versions of Python that you need, and that does not depend on Python itself, so there’s no bootstrap problem. But it is only available for Linux and macOS: Windows users should install pyenv-win: the following subsection deals with installing pyenv on Linux and macOS, the next one with installing pyenv-win for Windows.\n\n1.2.1 pyenv for Linux or macOS\nI recommend you read and follow the official instructions1, but I provide here the main steps. In case something doesn’t work as expected, refer to the official documentation linked previously. First, install the build dependencies by following the instructions here2. On a Linux distribution like Ubuntu, this means installing a bunch of development libraries from your usual package manager. On macOS, this means installing Xcode and also a bunch of packages with Homebrew. Also, make sure you have installed Git as well. We will be using Git later in the book too, but it is required to install pyenv.\nThen, on macOS, you can use Hombrew to install pyenv:\nbrew update\nbrew install pyenv\nFor Linux distributions, run the automatic installer by opening a terminal and executing this line here:\ncurl https://pyenv.run | bash\nNow comes the more complex part of the installation process. You need to edit some files for your terminal, and these files are different depending on which shell you use. For most Linux distributions, if not all, the default terminal will use bash, and on macOS the default shell is zsh. The detailled instructions, which I recommend you read, are here3, but here are my recommendations: if you’re using a Linux distribution, you likely are using bash, to find out, run echo $0 in a terminal. If your terminal is using bash, “bash” will get printed, if not, “zsh” will get printed instead. If your terminal using bash, run the following in a terminal:\necho 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt; ~/.bashrc\necho 'command -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"' &gt;&gt; ~/.bashrc\necho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.bashrc\nand then, you need to check whether you have a .profile, .bash_profile or a .bash_login file in your home directory. These are files used at startup by bash to set some variables and options. They’re similar to the .bashrc file, but the .bashrc is executed when starting a terminal when you’re already logged into the system, while .bash_profile, .bash_login and .profile get executed when logging in through ssh into the system. If you have a .profile file in your HOME, this should return something:\nls .profile\nif nothing gets returned, you might have instead a .bash_profile or bash_login file instead, try with:\nls .bash_profile\nor\nls .bash_login\nIf you have more than one, only add the lines to the .bash_profile:\necho 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt; ~/.bash_profile\necho 'command -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"' &gt;&gt; ~/.bash_profile\necho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.bash_profile\nbut if you have only have a .bash_login file, run this:\necho 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt; ~/.bash_login\necho 'command -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"' &gt;&gt; ~/.bash_login\necho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.bash_login\nor if you only have a .profile file, or none of these three files at all, run these lines:\necho 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt; ~/.profile\necho 'command -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"' &gt;&gt; ~/.profile\necho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.profile\nIf you’re running zsh (which would be the case on macOS, or if you changed form bash to zsh on Linux), then you need to add these lines to the .zshrc file:\necho 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt; ~/.zshrc\necho '[[ -d $PYENV_ROOT/bin ]] && export PATH=\"$PYENV_ROOT/bin:$PATH\"' &gt;&gt; ~/.zshrc\necho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.zshrc\nand also to .zprofile or .zlogin, depending on which you have (if you have none, put the lines into .zprofile):\necho 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt; ~/.zprofile\necho '[[ -d $PYENV_ROOT/bin ]] && export PATH=\"$PYENV_ROOT/bin:$PATH\"' &gt;&gt; ~/.zprofile\necho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.zprofile\nIf you’re using another shell, such as fish, you can checkout the instructions on pyenv’s GitHub repository, or simply add these lines in the equivalent startup files.\nFinally, also add this line to your .bashrc or .zshrc:\neval \"$(pyenv virtualenv-init -)\"\nto load pyenv-virtualenv automatically (as stated by the installer).\n\n\n1.2.2 pyenv-win for Windows\nIf you’re using Windows, you should install pyenv-win instead, which you can find here4. Installation on Windows is similar to how you install pyenv on Linux, if you choose the PowerShell method. It’s simply running this line (see here5) in Powershell:\nInvoke-WebRequest -UseBasicParsing -Uri \"https://raw.githubusercontent.com/pyenv-win/pyenv-win/master/pyenv-win/install-pyenv-win.ps1\" -OutFile \"./install-pyenv-win.ps1\"; &\"./install-pyenv-win.ps1\"\nOpen a new PowerShell prompt and run pyenv. If you see a “command not found” error, you need to manually add pyenv to the PATH. On Windows, this is done graphically. Click on the start menu, and look for “Edit environment variables for your account”.\n\n\n\n\n\nLook for ‘Edit environment variables for your account’\n\n\n\n\nThen, click on the “New…” button, and the PYENV, PYENV_HOME and PYENV_ROOT. Make sure to adjus the paths to the ones on your machine.\n\n\n\n\n\nMake sure your environment variables are correctly set\n\n\n\n\nAlso check the FAQ6 if you need further details.\n\n\n1.2.3 Using pyenv\nNow that you have pyenv installed, we can use it to install as many different Python versions that we need, typically one per project. First, I’ll install a version of Python that I want to use. Let’s got with the latest available. Let’s see what is available:\npyenv install -l\nAs of writing, the latest version that is not in beta is 3.10.5, so let’s install this one (if you’re reading this from the future and a new version of Python is available, install 3.10.5 nonetheless):\npyenv install 3.10.5\nthis will install Python 3.10.5. But how do we use it now? Let’s suppose that we want to use it for a project. Let’s create a folder that will contain the different scripts that we are going to write in the next chapter and called it housing.\nOpen a terminal inside that folder, or open a terminal and cd into that folder:\ncd ~/home/Documents/housing\n(depending on your operating system, you could first open the folder, then right click somewhere empty and then click on Open in terminal, which would open a terminal in that folder). We would like to use Python 3.10.5 for this project, and we would like to do so each time we interact with the files in that folder. We need to set version 3.10.5 as the local Python version, meaning, the Python version that will be used for this project only:\npyenv local 3.10.5\nThis will generate a .python-version with the following line in it:\n3.10.5\nThis file will only be used by pyenv, not pipenv, but it’s nice to keep it as a reminder of which Python version you actually need for this project. pipenv specifies the Python version in its own file, which I will discuss in the next section."
  },
  {
    "objectID": "setting_up.html#second-step-pipenv",
    "href": "setting_up.html#second-step-pipenv",
    "title": "1  Setting up a development environment",
    "section": "1.3 Second step: pipenv",
    "text": "1.3 Second step: pipenv\nWe can now use our local Python interpreter to install pipenv:\npip install --user pipenv\nWe can now use pipenv to install the packages for our project. But why don’t we just keep using pip instead of pipenv? Why introduce yet another tool? In my opinion, pipenv has one absolutely crucial feature for reproducibility: pipenv enables deterministic builds, which means that when using pipenv, we will always get exactly the same packages installed.\n“But isn’t that exctaly what using requirements.txt file does?” you wonder. You are not entirely wrong. After all, if you make the effort to specify the packages you need, and their versions, wouldn’t running pip install -r requirements.txt also install exactly the same packages?\nWell, not quite. Imagine for example that you need a package called hello and you put it into your requirements.txt like so:\nhello==1.0.1\nSuppose that hello depends on another package called ciao. If you run pip install -r requirements.txt today, you’ll get hello at version 1.0.1 and ciao, say, at version 0.3.2. But if you run pip install -r requirements.txt in 6 months, you would still get hello at version 1.0.1 but you might get a newer version of ciao. This is because ciao itself is not specified in the requirements.txt, unless you made sure to add it (and then also add its dependencies, and their dependencies…). pipenv takes care of this for you, and adds further security checks by comparing sha256 hashes from the lock file to the ones from the downloaded packages, making sure that you are actually installing what you believe you are.\nNow that pipenv is installed, let’s start using it to install the packages we need for our project:\npipenv --python 3.10.5 install beautifulsoup4==4.12.2 pandas==2.1.4 plotnine==0.12.4 skimpy==0.0.11\n(Little sidenote: in Chapter 2, we will be working together on a project using real data with polars instead of pandas. But for now, let’s get pandas installed: I’ll do a short comparison of the two, and then we’ll keep on using polars for the remainder of the book.)\nAs you can see, I chose to install specific versions of packages. This is because I want you to follow along with the same versions as in the book. You could remove the ==x.y.z strings from the command above to install the latest versions of the packages available if you prefer, but then there would be no guarantee that you would find the same results as I do in the remainder of the book. Also, if you don’t specify versions, the Pipfile file won’t specify them either, only the Pipfile.lock will, which in certain cases could lead to undesired behaviour. I won’t go into specifics, because I highly recommend you always take the time to specify package versions when installing. Check package versions on Pypi. For example, here7 is the page to check out versions of the pandas package.\nYou should see now two new files in the housing folder, Pipfile and Pipfile.lock. Start by opening Pipfile, it should look like this:\n[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\nbeautifulsoup4 = \"==4.12.2\"\npandas = \"==2.1.4\"\nplotnine = \"==0.12.4\"\nskimpy = \"==0.0.11\"\n\n[dev-packages]\n\n[requires]\npython_version = \"3.10\"\npython_full_version = \"3.10.5\"\nI think that this file is pretty self-evident: the packages being used for this project are listed alongside their versions. The exact Python version is also listed. Sometimes, depending on how you set up the project, it could happen that the Python version listed is not the same, or that only the field python_verison= is listed. In this case, I highly recommend you change it to match the same version as in the .python-version file that was generated by pyenv, and add the python_full_version= field if necessary, in order to avoid any possible differences in how the code runs. If you edited the Pipfile then you need to run pipenv lock to regenerate the Pipfile.lock file as well:\npipenv lock\nthis will make sure to also set the required/correct Python version in there.\nIf you open the Pipfile.lock in a text editor, you will see that it is a json file and that is also lists the dependecies of your project, but also the dependencies’ dependencies. You will also notice several fields called hashes. These are there for security reasons: whenever someone, (or you in the future) will regenerate this environment, the packages will get downloaded and their hashes will get compared to the ones listed in the Pipfile.lock. If they don’t match, then something very wrong is happening and packages won’t get installed.\nTo check whether everything installed correctly, drop into the development shell using:\npipenv shell\nand start the Python interpreter:\npython\nThen check that the correct versions of the packages were installed:\nimport pandas as pd\npd.__version__\nYou should see 2.1.4 as the listed version."
  },
  {
    "objectID": "setting_up.html#debugging",
    "href": "setting_up.html#debugging",
    "title": "1  Setting up a development environment",
    "section": "1.4 Debugging",
    "text": "1.4 Debugging\nIt can sometimes happen that pipenv does not use the right version of Python. Running pipenv shell and then starting the Python interpreter will start a different version of Python than the one you expect/need.\nIn that case, it might be best to simply start over. Start by completely removing the virtual environment and packages by running:\npipenv --rm\nMake sure that the correct version of Python (or rather, the version you need) is specified in the Pipfile. For example, in my case:\n[requires]\npython_full_version = \"3.10.5\"\n(you might instead see python_version = \"3.10\", or any other version, in that case I recommend you specify python_full_version as I did above, check the .python-version file that was generated by pyenv if you don’t remember the right version).\nNow, restore the environment and packages by running:\npipenv install\npipenv install will regenerate the Pipfile.lock file to list the required Python version, and then install the packages. It essentially runs pipenv lock to generate the lock file, and then pipenv sync to install the packages from the lock file.\nCheck if pipenv now uses the correct Python version.\npipenv run python --version"
  },
  {
    "objectID": "setting_up.html#a-high-level-description-of-how-to-set-up-a-project",
    "href": "setting_up.html#a-high-level-description-of-how-to-set-up-a-project",
    "title": "1  Setting up a development environment",
    "section": "1.5 A high-level description of how to set up a project",
    "text": "1.5 A high-level description of how to set up a project\nOk, so to summarise, we install pyenv which will make it easy to install any version of Python that we require, and then we installed pipenv, which we use to install packages. The advantage of using pipenv is that we get deterministic builds, and pipenv works well with pyenv to build project-specific environments.\nThe way I would suggest you use these tools now, is that for each project, you install the latest available version of Python, define it as the local version using pyenv local, and then install packages by specyifing their versions, like so:\npipenv --python 3.10.5 install beautifulsoup4==4.12.2 pandas==2.1.4 plotnine==0.12.4 skimpy==0.0.11\n(Replace the versions of Python and packages by the latest, or those you need.)\nThis will ensure that your project uses the correct software stack, and that collaborators or future you will be able to regenerate this environment by calling pipenv sync. pipenv sync differs from pipenv install because it will not touch the Pipfile.lock file, and will just download the packages as listed in the lock file. This is also the command that we will use later, in the chapter on CI/CD."
  },
  {
    "objectID": "setting_up.html#footnotes",
    "href": "setting_up.html#footnotes",
    "title": "1  Setting up a development environment",
    "section": "",
    "text": "https://github.com/pyenv/pyenv?tab=readme-ov-file#installation↩︎\nhttps://github.com/pyenv/pyenv/wiki#suggested-build-environment↩︎\nhttps://github.com/pyenv/pyenv?tab=readme-ov-file#set-up-your-shell-environment-for-pyenv↩︎\nhttps://github.com/pyenv-win/pyenv-win↩︎\nhttps://github.com/pyenv-win/pyenv-win/blob/master/docs/installation.md#powershell↩︎\nhttps://github.com/pyenv-win/pyenv-win/wiki↩︎\nhttps://pypi.org/project/pandas/↩︎"
  },
  {
    "objectID": "project_start.html#the-polars-package-and-why-you-should-ditch-pandas-in-its-favour",
    "href": "project_start.html#the-polars-package-and-why-you-should-ditch-pandas-in-its-favour",
    "title": "2  Project start",
    "section": "2.1 The Polars package and why you should ditch Pandas in its favour",
    "text": "2.1 The Polars package and why you should ditch Pandas in its favour"
  },
  {
    "objectID": "project_start.html#housing-in-luxembourg",
    "href": "project_start.html#housing-in-luxembourg",
    "title": "2  Project start",
    "section": "2.2 Housing in Luxembourg",
    "text": "2.2 Housing in Luxembourg\nWe are going to download data about house prices in Luxembourg. Luxembourg is a little Western European country the author hails from that looks like a shoe and is about the size of .98 Rhode Islands. Did you know that Luxembourg is a constitutional monarchy, and not a kingdom like Belgium, but a Grand-Duchy, and actually the last Grand-Duchy in the World? Also, what you should know to understand what we will be doing is that the country of Luxembourg is divided into Cantons, and each Cantons into Communes. If Luxembourg was the USA, Cantons would be States and Communes would be Counties (or Parishes or Boroughs). What’s confusing is that “Luxembourg” is also the name of a Canton, and of a Commune, which also has the status of a city and is the capital of the country. So Luxembourg the country, is divided into Cantons, one of which is called Luxembourg as well, cantons are divided into communes, and inside the canton of Luxembourg, there’s the commune of Luxembourg which is also the city of Luxembourg, sometimes called Luxembourg City, which is the capital of the country.\n\n\n\nLuxembourg is about as big as the US State of Rhode Island.\n\n\nWhat you should also know is that the population is about 645,000 as of writing (January 2023), half of which are foreigners. Around 400,000 persons work in Luxembourg, of which half do not live in Luxembourg; so every morning from Monday to Friday, 200,000 people enter the country to work and then leave in the evening to go back to either Belgium, France or Germany, the neighbouring countries. As you can imagine, this puts enormous pressure on the transportation system and on the roads, but also on the housing market; everyone wants to live in Luxembourg to avoid the horrible daily commute, and everyone wants to live either in the capital city, or in the second largest urban area in the south, in a city called Esch-sur-Alzette.\nThe plot below shows the value of the House Price Index over time for Luxembourg and the European Union:\n\n\n\n\n\nIf you want to download the data, click here1.\nLet us paste the definition of the HPI in here (taken from the HPI’s metadata2 page):\nThe House Price Index (HPI) measures inflation in the residential property market. The HPI captures price changes of all types of dwellings purchased by households (flats, detached houses, terraced houses, etc.). Only transacted dwellings are considered, self-build dwellings are excluded. The land component of the dwelling is included.\nSo from the plot, we can see that the price of dwellings more than doubled between 2010 and 2021; the value of the index is 214.81 in 2021 for Luxembourg, and 138.92 for the European Union as a whole.\nThere is a lot of heterogeneity though; the capital and the communes right next to the capital are much more expensive than communes from the less densely populated north, for example. The south of the country is also more expensive than the north, but not as much as the capital and surrounding communes. Not only is price driven by demand, but also by scarcity; in 2021, 0.5% of residents owned 50% of the buildable land for housing purposes (Source: Observatoire de l’Habitat, Note 29, archived download link3).\nOur project will be quite simple; we are going to download some data, supplied as an Excel file, compiled by the Housing Observatory (Observatoire de l’Habitat, a service from the Ministry of Housing, which monitors the evolution of prices in the housing market, among other useful services like the identification of vacant lots). The advantage of their data when compared to Eurostat’s data is that the data is disaggregated by commune. The disadvantage is that they only supply nominal prices, and no index (and the data is trapped inside Excel and not ready for analysis with R). Nominal prices are the prices that you read on price tags in shops. The problem with nominal prices is that it is difficult to compare them through time. Ask yourself the following question: would you prefer to have had 500€ (or USDs) in 2003 or in 2023? You probably would have preferred them in 2003, as you could purchase a lot more with $500 then than now. In fact, according to a random inflation calculator I googled, to match the purchasing power of $500 in 2003, you’d need to have $793 in 2023 (and I’d say that we find very similar values for €). But it doesn’t really matter if that calculation is 100% correct: what matters is that the value of money changes, and comparisons through time are difficult, hence why an index is quite useful. So we are going to convert these nominal prices to real prices. Real prices take inflation into account and so allow us to compare prices through time.\nSo to summarise; our goal is to:\n\nGet data trapped inside an Excel file into a neat data frame;\nConvert nominal to real prices using a simple method;\nMake some tables and plots and call it a day (for now).\n\nWe are going to start in the most basic way possible; we are simply going to write a script and deal with each step separately."
  },
  {
    "objectID": "project_start.html#saving-trapped-data-from-excel",
    "href": "project_start.html#saving-trapped-data-from-excel",
    "title": "2  Project start",
    "section": "2.3 Saving trapped data from Excel",
    "text": "2.3 Saving trapped data from Excel\nGetting data from Excel into a tidy data frame can be very tricky. This is because very often, Excel is used as some kind of dashboard or presentation tool. So data is made human-readable, in contrast to machine-readable. Let us quickly discuss this topic as it is essential to grasp the difference between the two (and in our experience, a lot of collective pain inflicted to statisticians and researchers could have been avoided if this concept was more well-known). The picture below shows an Excel file made for human consumption:\n\n\n\nAn Excel file meant for human eyes.\n\n\nSo why is this file not machine-readable? Here are some issues:\n\nThe table does not start in the top-left corner of the spreadsheet, which is where most importing tools expect it to be;\nThe spreadsheet starts with a header that contains an image and some text;\nNumbers are text and use “,” as the thousands separator;\nYou don’t see it in the screenshot, but each year is in a separate sheet.\n\nThat being said, this Excel file is still very tame, and going from this Excel to a tidy data frame will not be too difficult. In fact, we suspect that whoever made this Excel file is well aware of the contradicting requirements of human and machine-readable formatting of data, and strove to find a compromise. Because more often than not, getting human-readable data into a machine-readable format is a nightmare. We could call data like this machine-friendly data.\nIf you want to follow along, you can download the Excel file here4 (downloaded on January 2023 from the luxembourguish open data portal5). But you don’t need to follow along with code, because I will link the completed scripts for you to download later.\nEach sheet contains a dataset with the following columns:\n\nCommune: the commune (the smallest administrative division of territory);\nNombre d’offres: the total number of selling offers;\nPrix moyen annoncé en Euros courants: Average selling price in nominal Euros;\nPrix moyen annoncé au m2 en Euros courants: Average selling price in square meters in nominal Euros.\n\nFor ease of presentation, I’m going to show you each step of the analysis here separately, but I’ll be putting everything together in a single script once I’m done explaining each step. So first, let’s load some packages:\n\nimport polars as pl\nimport polars.selectors as cs\nimport re\n\nI will be using the polars package to manipulate data.\nNext, the code below downloads the data, and puts it in a data frame:\n\n# The url below points to an Excel file\n# hosted on the book’s github repository\nurl &lt;- \"https://is.gd/1vvBAc\"\n\nraw_data &lt;- tempfile(fileext = \".xlsx\")\n\ndownload.file(url, raw_data,\n              method = \"auto\",\n              mode = \"wb\")\n\nsheets &lt;- excel_sheets(raw_data)\n\nread_clean &lt;- function(..., sheet){\n  read_excel(..., sheet = sheet) |&gt;\n    mutate(year = sheet)\n}\n\nraw_data &lt;- map(\n  sheets,\n  ~read_clean(raw_data,\n              skip = 10,\n              sheet = .)\n                   ) |&gt;\n  bind_rows() |&gt;\n  clean_names()\n\nraw_data &lt;- raw_data |&gt;\n  rename(\n    locality = commune,\n    n_offers = nombre_doffres,\n    average_price_nominal_euros = prix_moyen_annonce_en_courant,\n    average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant,\n    average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant\n  ) |&gt;\n  mutate(locality = str_trim(locality)) |&gt;\n  select(year, locality, n_offers, starts_with(\"average\"))\n\nIf you are familiar with the {tidyverse} (Wickham et al. 2019) the above code should be quite easy to follow. We start by downloading the raw Excel file and saving the sheet names into a variable. We then use a function called read_clean(), which takes the path to the Excel file and the sheet names as an argument to read the required sheet into a data frame. We use skip = 10 to skip the first 10 lines in each Excel sheet because the first 10 lines contain a header. The last thing this function does is add a new column called year which contains the year of the data. We’re lucky because the sheet names are the years: “2010”, “2011” and so on. We then map this function to the list of sheet names, thus reading in all the data from all the sheets into one list of data frames. We then use bind_rows(), to bind each data frame into a single data frame, by row. Finally, we rename the columns (by translating their names from French to English) and only select the required columns. If you don’t understand each step of what is going on, don’t worry too much about it; this book is not about learning how to use R.\nRunning this code results in a neat data set:\n\nraw_data\n\nBut there’s a problem: columns that should be of type numeric are of type character instead (average_price_nominal_euros and average_price_m2_nominal_euros). There’s also another issue, which you would eventually catch as you’ll explore the data: the naming of the communes is not consistent. Let’s take a look:\n\nraw_data |&gt;\n  filter(grepl(\"Luxembourg\", locality)) |&gt;\n  count(locality)\n\nWe can see that the city of Luxembourg is spelled in two different ways. It’s the same with another commune, Pétange:\n\nraw_data |&gt;\n  filter(grepl(\"P.tange\", locality)) |&gt;\n  count(locality)\n\nSo sometimes it is spelled correctly, with an “é”, sometimes not. Let’s write some code to correct both these issues:\n\nraw_data &lt;- raw_data |&gt;\n  mutate(\n    locality = ifelse(grepl(\"Luxembourg-Ville\", locality),\n                      \"Luxembourg\",\n                      locality),\n         locality = ifelse(grepl(\"P.tange\", locality),\n                           \"Pétange\",\n                           locality)\n         ) |&gt;\n  mutate(across(starts_with(\"average\"),\n         as.numeric))\n\nNow this is interesting – converting the average columns to numeric resulted in some NA values. Let’s see what happened:\n\nraw_data |&gt;\n  filter(is.na(average_price_nominal_euros))\n\nIt turns out that there are no prices for certain communes, but that we also have some rows with garbage in there. Let’s go back to the raw data to see what this is about:\n\n\n\nAlways look at your data.\n\n\nSo it turns out that there are some rows that we need to remove. We can start by removing rows where locality is missing. Then we have a row where locality is equal to “Total d’offres”. This is simply the total of every offer from every commune. We could keep that in a separate data frame, or even remove it. The very last row states the source of the data and we can also remove it. Finally, in the screenshot above, we see another row that we don’t see in our filtered data frame: one where n_offers is missing. This row gives the national average for columns average_prince_nominal_euros and average_price_m2_nominal_euros. What we are going to do is create two datasets: one with data on communes, and the other on national prices. Let’s first remove the rows stating the sources:\n\nraw_data &lt;- raw_data |&gt;\n  filter(!grepl(\"Source\", locality))\n\nLet’s now only keep the communes in our data:\n\ncommune_level_data &lt;- raw_data |&gt;\n    filter(!grepl(\"nationale|offres\", locality),\n           !is.na(locality))\n\nAnd let’s create a dataset with the national data as well:\n\ncountry_level &lt;- raw_data |&gt;\n  filter(grepl(\"nationale\", locality)) |&gt;\n  select(-n_offers)\n\noffers_country &lt;- raw_data |&gt;\n  filter(grepl(\"Total d.offres\", locality)) |&gt;\n  select(year, n_offers)\n\ncountry_level_data &lt;- full_join(country_level, offers_country) |&gt;\n  select(year, locality, n_offers, everything()) |&gt;\n  mutate(locality = \"Grand-Duchy of Luxembourg\")\n\nNow the data looks clean, and we can start the actual analysis… or can we? Before proceeding, it would be nice to make sure that we got every commune in there. For this, we need a list of communes from Luxembourg. Thankfully, Wikipedia has such a list6.\nAn issue with scraping tables off the web is that they might change in the future. It is therefore a good idea to save the page by right clicking on it and then selecting save as, and then re-hosting it. I use Github pages to re-host the Wikipedia page above here7. I now have full control of this page, and won’t get any bad surprises if someone decides to eventually update it. Instead of re-hosting it, you could simply save it as any other file of your project.\nSo let’s scrape and save this list:\n\ncurrent_communes &lt;- \"https://is.gd/lux_communes\" |&gt;\n  rvest::read_html() |&gt;\n  rvest::html_table() |&gt;\n  purrr::pluck(2) |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::filter(name_2 != \"Name\") |&gt;\n  dplyr::rename(commune = name_2) |&gt;\n  dplyr::mutate(commune = stringr::str_remove(commune, \" .$\"))\n\nWe scrape the table from the re-hosted Wikipedia page using {rvest}. rvest::html_table() returns a list of tables from the Wikipedia table, and then we use purrr::pluck() to keep the second table from the website, which is what we need (I made the calls to the packages explicit, because you might not be familiar with these packages). janitor::clean_names() transforms column names written for human eyes into machine-friendly names (for example Growth rate in % would be transformed to growth_rate_in_percent) and then I use the {dplyr} package for some further cleaning and renaming; the very last step removes a dagger symbol next to certain communes names, in other words it turns “Commune †” into “Commune”.\nLet’s see if we have all the communes in our data:\n\nsetdiff(unique(commune_level_data$locality),\n        current_communes$commune)\n\nWe see many communes that are in our commune_level_data, but not in current_communes. There’s one obvious reason: differences in spelling, for example, “Kaerjeng” in our data, but “Käerjeng” in the table from Wikipedia. But there’s also a less obvious reason; since 2010, several communes have merged into new ones. So there are communes that are in our data in 2010 and 2011, but disappear from 2012 onwards. So we need to do several things: first, get a list of all existing communes from 2010 onwards, and then, harmonise spelling. Here again, we can use a list from Wikipedia, and here again, I decide to re-host it on Github pages to avoid problems in the future:\n\nformer_communes &lt;- \"https://is.gd/lux_former_communes\" |&gt;\n  rvest::read_html() |&gt;\n  rvest::html_table() |&gt;\n  purrr::pluck(3) |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::filter(year_dissolved &gt; 2009)\n\nformer_communes\n\nAs you can see, since 2010 many communes have merged to form new ones. We can now combine the list of current and former communes, as well as harmonise their names:\n\ncommunes &lt;- unique(c(former_communes$name,\n                     current_communes$commune))\n# we need to rename some communes\n\n# Different spelling of these communes between wikipedia and the data\n\ncommunes[which(communes == \"Clemency\")] &lt;- \"Clémency\"\ncommunes[which(communes == \"Redange\")] &lt;- \"Redange-sur-Attert\"\ncommunes[which(communes == \"Erpeldange-sur-Sûre\")] &lt;- \"Erpeldange\"\ncommunes[which(communes == \"Luxembourg City\")] &lt;- \"Luxembourg\"\ncommunes[which(communes == \"Käerjeng\")] &lt;- \"Kaerjeng\"\ncommunes[which(communes == \"Petange\")] &lt;- \"Pétange\"\n\nLet’s run our test again:\n\nsetdiff(unique(commune_level_data$locality),\n        communes)\n\nGreat! When we compare the communes that are in our data with every commune that has existed since 2010, we don’t have any commune that is unaccounted for. So are we done with cleaning the data? Yes, we can now start with analysing the data. Take a look here8 to see the finalised script. Also read some of the comments that I’ve added. This is a typical R script, and at first glance, one might wonder what is wrong with it. Actually, not much, but the problem if you leave this script as it is, is that it is very likely that we will have problems rerunning it in the future. As it turns out, this script is not reproducible. But we will discuss this in much more detail later on. For now, let’s analyse our cleaned data."
  },
  {
    "objectID": "project_start.html#analysing-the-data",
    "href": "project_start.html#analysing-the-data",
    "title": "2  Project start",
    "section": "2.4 Analysing the data",
    "text": "2.4 Analysing the data\nWe are now going to analyse the data. The first thing we are going to do is compute a Laspeyeres price index. This price index allows us to make comparisons through time; for example, the index at year 2012 measures how much more expensive (or cheaper) housing became relative to the base year (2010). However, since we only have one ‘good’ (housing), this index becomes quite simple to compute: it is nothing but the prices at year t divided by the prices in 2010 (if we had a basket of goods, we would need to use the Laspeyeres index formula to compute the index at all periods).\nFor this section, I will perform a rather simple analysis. I will immediately show you the R script: take a look at it here9. For the analysis I selected 5 communes and plotted the evolution of prices compared to the national average.\nThis analysis might seem trivially simple, but it contains all the needed ingredients to illustrate everything else that I’m going to teach you in this book.\nMost analyses would stop here: after all, we have what we need; our goal was to get the plots for the 5 communes of Luxembourg, Esch-sur-Alzette, Mamer, Schengen (which gave its name to the Schengen Area10) and Wincrange. However, let’s ask ourselves the following important questions:\n\nHow easy would it be for someone else to rerun the analysis?\nHow easy would it be to update the analysis once new data gets published?\nHow easy would it be to reuse this code for other projects?\nWhat guarantee do we have that if the scripts get run in 5 years, with the same input data, we get the same output?\n\nLet’s answer these questions one by one."
  },
  {
    "objectID": "project_start.html#your-project-is-not-done",
    "href": "project_start.html#your-project-is-not-done",
    "title": "2  Project start",
    "section": "2.5 Your project is not done",
    "text": "2.5 Your project is not done\n\n2.5.1 How easy would it be for someone else to rerun the analysis?\nThe analysis is composed of two R scripts, one to prepare the data, and another to actually run the analysis proper. Performing the analysis might seem quite easy, because each script contains comments as to what is going on, and the code is not that complicated. However, we are missing any project-level documentation that would provide clear instructions as to how to run the analysis. This might seem simple for us who wrote these scripts, but we are familiar with R, and this is still fresh in our brains. Should someone less familiar with R have to run the script, there is no clue for them as to how they should do it. And of course, should the analysis be more complex (suppose it’s composed of dozens of scripts), this gets even worse. It might not even be easy for you to remember how to run this in 5 months!\nAnd what about the required dependencies? Many packages were used in the analysis. How should these get installed? Ideally, the same versions of the packages you used and the same version of R should get used by that person to rerun the analysis.\nAll of this still needs to be documented, but listing the packages that were used for an analysis and their versions takes quite some time. Thankfully, in part 2, we will learn about the {renv} package to deal with this in a couple lines of code.\n\n\n2.5.2 How easy would it be to update the project?\nIf new data gets published, all the points discussed previously are still valid, plus you need to make sure that the updated data is still close enough to the previous data such that it can pass through the data cleaning steps you wrote. You should also make sure that the update did not introduce a mistake in past data, or at least alert you if that is the case. Sometimes, when new years get added, data for previous years also get corrected, so it would be nice to make sure that you know this. Also, in the specific case of our data, communes might get fused into a new one, or maybe even divided into smaller communes (even though this has not happened in a long time, it is not entirely out of the question).\nIn summary, what is missing from the current project are enough tests to make sure that an update to the data can happen smoothly.\n\n\n2.5.3 How easy would it be to reuse this code for another project?\nSaid plainly, not very easy. With code in this state you have no choice but to copy and paste it into a new script and change it adequately. For re-usability, nothing beats structuring your code into functions and ideally you would even package them. We are going to learn just that in future chapters of this book.\nBut sometimes you might not be interested in reusing code for another project: however, even if that’s the case, structuring your code into functions and packaging them makes it easy to reuse code even inside the same project. Look at the last part of the analysis.R script: we copied and pasted the same code 5 times and only slightly changed it. We are going to learn how not to repeat ourselves by using functions and you will immediately see the benefits of writing functions, even when simply reusing them inside the same project.\n\n\n2.5.4 What guarantee do we have that the output is stable through time?\nNow this might seem weird: after all, if we start from the same dataset, does it matter when we run the scripts? We should be getting the same result if we build the project today, in 5 months or in 5 years. Well, not necessarily. While it is true that R is quite stable, this cannot necessarily be said of the packages that we use. There is no guarantee that the authors of the packages will not change the package’s functions to work differently, or take arguments in a different order, or even that the packages will all be available at all in 5 years. And even if the packages are still available and function the same, bugs in the packages might get corrected which could alter the result. This might seem like a non-problem; after all, if bugs get corrected, shouldn’t you be happy to update your results as well? But this depends on what it is we’re talking about. Sometimes it is necessary to reproduce results exactly as they were, even if they were wrong, for example in the context of an audit.\nSo we also need a way to somehow snapshot and freeze the computational environment that was used to create the project originally."
  },
  {
    "objectID": "project_start.html#conclusion",
    "href": "project_start.html#conclusion",
    "title": "2  Project start",
    "section": "2.6 Conclusion",
    "text": "2.6 Conclusion\nWe now have a basic analysis that has all we need to get started. In the coming chapters, we are going to learn about topics that will make it easy to write code that is more robust, better documented and tested, and most importantly easy to rerun (and thus to reproduce the results). The first step will actually not involve having to start rewriting our scripts though; next, we are going to learn about Git, a tool that will make our life easier by versioning our code.\n\n\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686."
  },
  {
    "objectID": "project_start.html#footnotes",
    "href": "project_start.html#footnotes",
    "title": "2  Project start",
    "section": "",
    "text": "https://is.gd/AET0ir↩︎\nhttps://archive.is/OrQwA, archived link for posterity.↩︎\nhttps://archive.org/download/note-29/note-29.pdf↩︎\nhttps://is.gd/1vvBAc↩︎\nhttps://data.public.lu/en/datasets/prix-annonces-des-logements-par-commune/↩︎\nhttps://w.wiki/6nPu↩︎\nhttps://is.gd/lux_communes↩︎\nhttps://is.gd/7PhUjd↩︎\nhttps://is.gd/qCJEbi↩︎\nhttps://en.wikipedia.org/wiki/Schengen_Area↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686."
  }
]