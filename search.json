[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Building reproducible analytical pipelines with Python",
    "section": "",
    "text": "Welcome!",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#how-using-a-few-ideas-from-software-engineering-can-help-data-scientists-analysts-and-researchers-write-reliable-code",
    "href": "index.html#how-using-a-few-ideas-from-software-engineering-can-help-data-scientists-analysts-and-researchers-write-reliable-code",
    "title": "Building reproducible analytical pipelines with Python",
    "section": "How using a few ideas from software engineering can help data scientists, analysts and researchers write reliable code",
    "text": "How using a few ideas from software engineering can help data scientists, analysts and researchers write reliable code\nThis is the Python edition of the book “Building reproducible analytical pipelines”, and it’s a work-in-progress. If you’re looking for the R version, visit this link.\n\n\n \n\nData scientists, statisticians, analysts, researchers, and many other professionals write a lot of code.\nNot only do they write a lot of code, but they must also read and review a lot of code as well. They either work in teams and need to review each other’s code, or need to be able to reproduce results from past projects, be it for peer review or auditing purposes. And yet, they never, or very rarely, get taught the tools and techniques that would make the process of writing, collaborating, reviewing and reproducing projects possible.\nWhich is truly unfortunate because software engineers face the same challenges and solved them decades ago.\nThe aim of this book is to teach you how to use some of the best practices from software engineering and DevOps to make your projects robust, reliable and reproducible. It doesn’t matter if you work alone, in a small or in a big team. It doesn’t matter if your work gets (peer-)reviewed or audited: the techniques presented in this book will make your projects more reliable and save you a lot of frustration!\nAs someone whose primary job is analysing data, you might think that you are not a developer. It seems as if developers are these genius types that write extremely high-quality code and create these super useful packages. The truth is that you are a developer as well. It’s just that your focus is on writing code for your purposes to get your analyses going instead of writing code for others. Or at least, that’s what you think. Because in others, your team-mates are included. Reviewers and auditors are included. Any people that will read your code are included, and there will be people that will read your code. At the very least future you will read your code. By learning how to set up projects and write code in a way that future you will understand and not want to murder you, you will actually work towards improving the quality of your work, naturally.\nThe book can be read for free on https://b-rodrigues.github.io/raps_with_py/ and you’ll be able buy a DRM-free Epub or PDF on Leanpub1 once there’s more content.\nThis is the Python edition of my book titled Building reproducible analytical pipelines with R2. This means that a lot of text is copied over, but the all of the code and concepts are completely adapted to the Python programming language. This book is also shorter than the R version. Here’s the topics that I will cover:\n\nDependency management with pipenv;\nSome thoughts on functional programming with Python;\nUnit and assertive testing;\nBuild automation with ploomber;\nLiterate programming with Quarto;\nReproducible environments with Docker;\nContinuous integration and delivery.\n\nWhile this is not a book for beginners (you really should be familiar with Python before reading this), I will not assume that you have any knowledge of the tools discussed. But be warned, this book will require you to take the time to read it, and then type on your computer. Type a lot.\nI hope that you will enjoy reading this book and applying the ideas in your day-to-day, ideas which hopefully should improve the reliability, traceability and reproducibility of your code. You can read this book for free on TO UPDATE\nYou can also buy a physical copy of the book on Amazon.\nIf you want to get to know me better, read my bio3.\nYou’ll also be able to buy a physical copy of the book on Amazon once it’s done. In the meantime, you could buy the R edition.\nIf you find this book useful, don’t hesitate to let me know or leave a rating on Amazon!\nYou can submit issues, PRs and ask questions on the book’s Github repository4.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Building reproducible analytical pipelines with Python",
    "section": "",
    "text": "https://leanpub.com/↩︎\nhttps://raps-with-r.dev↩︎\nhttps://www.brodrigues.co/about/me/↩︎\nhttps://github.com/b-rodrigues/raps_with_py↩︎",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "preface.html#footnotes",
    "href": "preface.html#footnotes",
    "title": "Preface",
    "section": "",
    "text": "https://www.raps-with-r.dev↩︎\nhttps://rap4mads.eu/↩︎\nhttps://analysisfunction.civilservice.gov.uk/support/reproducible-analytical-pipelines/↩︎\nhttps://ukgovdatascience.github.io/rap_companion/↩︎\nhttps://software-carpentry.org/↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Who is this book for?\nThis book is for anyone that uses raw data to build any type of output based on that raw data. This can be a simple quarterly report for example, in which the data is used for tables and graphs, or a scientific article for a peer reviewed journal or even an interactive web application. It doesn’t matter, because the process is, at its core, always very similar:\nThis book will already assume some familiarity with programming, and in particular the Python programming language. As I’ve stated in the preface, I’m not a Python expert, but I’m comfortable enough with the language. In any case, this is not a book about programming itself, and bar a short discussion on the merits of the Polars package, I won’t be teaching you programming.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#who-is-this-book-for",
    "href": "intro.html#who-is-this-book-for",
    "title": "1  Introduction",
    "section": "",
    "text": "Get the data;\nClean the data;\nWrite code to analyse the data;\nPut the results into the final product.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-the-aim-of-this-book",
    "href": "intro.html#what-is-the-aim-of-this-book",
    "title": "1  Introduction",
    "section": "1.2 What is the aim of this book?",
    "text": "1.2 What is the aim of this book?\nThe aim of this book is to make the process of analysing data as reliable, retraceable, and reproducible as possible, and do this by design. This means that once you’re done with the analysis, you’re done. You don’t want to spend time, which you often don’t have anyways, to rewrite or refactor an analysis and make it reproducible after the fact. We both know that this is not going to happen. Once an analysis is done, it’s time to go to the next analysis. And if you need to rerun an older analysis (for example, because the data got updated), then you’ll simply figure it out at that point, right? That’s a problem for future you, right? Hopefully, future you will remember every quirk of your code and know which script to run at which point in the process, which comments are outdated and can be safely ignored, what features of the data need to be checked (and when they need to be checked), and so on… You better hope future you is a more diligent worker than you!\nGoing forward, I’m going to refer to a project that is reproducible as a “reproducible analytical pipeline”, or RAP for short. There are only two ways to make such a RAP; either you are lucky enough to have someone on the team whose job is to turn your messy code into a RAP, or you do it yourself. And this second option is very likely the most common. The issue is, as stated above, that most of us simply don’t do it. We are always in the rush to get to the results, and don’t think about making the process reproducible. This is because we always think that making the process reproducible takes time and this time is better spent working on the analysis itself. But this is a misconception, for two reasons.\nThe first reason is that employing the techniques that we are going to discuss in this book won’t actually take much time. As you will see, they’re not really things that you “add on top of the analysis”, but will be part of the analysis itself, and they will also help with managing the project. And some of these techniques will even save you time (especially testing) and headaches.\nThe second reason is that an analysis is never, ever, a one-shot. Only the most simple things, like pulling out a number from some data base may be a one-shot. And even then, chances are that once you provide that number, you’ll be asked to pull out a variation of that number (for example, by disaggregating by one or several variables). Or maybe you’ll get asked for an update to that number in six months. So you will learn very quickly to keep that SQL query in a script somewhere to make sure that you provide a number that is consistent. But what about more complex analyses? Is keeping the script enough? Keeping the script is already a good start of course. The problem is that very often, there is no script, or not a script for each step of the analysis.\nI’ve seen this play out many times in many different organisations. It’s that time of the year again, we have to write a report. 10 people are involved, and just gathering the data is already complicated. Some get their data from Word documents attached to emails, some from a website, some from a report from another department that is a PDF… I remember a story that a senior manager at my previous job used to tell us: once, a client put out a call for a project that involved helping them setting up a PDF scraper. They periodically needed data from another department that came in PDFs. The manager asked what was, at least from our perspective, an obvious question: why can’t they send you the underlying data from that PDF in a machine readable format? They had never thought to ask. So my manager went to that department, and talked to the people putting that PDF together. Their answer? “Well, we could send them the data in any format they want, but they’ve asked us to send the tables in a PDF format”.\nSo the first, and probably most important lesson here is: when starting to build a RAP, make sure that you talk with all the people involved.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "1  Introduction",
    "section": "1.3 Prerequisites",
    "text": "1.3 Prerequisites\nYou should be comfortable with the Python programming language. This book will assume that you have been using Python for some projects already, and want to improve not only your knowledge of the language itself, but also how to successfully manage complex projects. Ideally, you should know about packages, how to install them, you should have written some functions already, know about loops and have some basic knowledge of data structures like lists. While this is not a book on visualisation, we will be making some graphs using the plotnine package, so if you’re familiar with that, that’s good. If not, no worries, visualisation, data munging or data analysis is not the point of this book. Chapter 2, Before we start should help you gauge how easily you will be able to follow this book.\nIdeally, you should also not be afraid of not using Graphical User Interfaces (GUIs). While you can follow along using an IDE like VS Code, I will not be teaching any features from any program with a GUI. This is not to make things harder than they should be (quite the contrary actually) but because interacting graphically with a program is simply not reproducible. So our aim is to write code that can be executed non-interactively by a machine. This is because one necessary condition for a workflow to be reproducible and get referred to as a RAP, is for the workflow to be able to be executed by a machine, automatically, without any human intervention. This is the second lesson of building RAPs: there should be no human intervention needed to get the outputs once the RAP is started. If you achieve this, then your workflow is likely reproducible, or can at least be made reproducible much more easily than if it requires some special manipulation by a human somewhere in the loop.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-actually-is-reproducibility",
    "href": "intro.html#what-actually-is-reproducibility",
    "title": "1  Introduction",
    "section": "1.4 What actually is reproducibility?",
    "text": "1.4 What actually is reproducibility?\nA reproducible project means that this project can be rerun by anyone at 0 (or very minimal) cost. But there are different levels of reproducibility, and I will discuss this in the next section. Let’s first discuss some requirements that a project must have to be considered a RAP.\n\n1.4.1 Using open-source tools to build a RAP is a hard requirement\nOpen source is a hard requirement for reproducibility.\nNo ifs nor buts. And I’m not only talking about the code you typed for your research paper/report/analysis. I’m talking about the whole ecosystem that you used to type your code and build the workflow.\nIs your code open? That’s good. Or is it at least available to other people from your organisation, in a way that they could re-execute it if needed? Good.\nBut is it code written in a proprietary program, like STATA, SAS or MATLAB? Then your project is not reproducible. It doesn’t matter if this code is well documented and written and available on a version control system (internally to your company or open to the public). This project is just not reproducible. Why?\nBecause on a long enough time horizon, there is no way to re-execute your code with the exact same version of the proprietary programming language and on the exact same version of the operating system that was used at the time the project was developed. As I’m writing these lines, MATLAB, for example, is at version R2022b. And buying an older version may not be simple. I’m sure if you contact their sales department they might be able to sell you an older version. Maybe you can even simply re-download older versions that you’ve already bought from their website. But maybe it’s not that simple. Or maybe they won’t offer this option anymore in the future, who knows? In any case, if you google “purchase old version of Matlab” you will see that many researchers and engineers have this need.\n\n\n\nWanting to run older versions of analytics software is a recurrent need.\n\n\nAnd if you’re running old code written for version, say, R2008a, there’s no guarantee that it will produce the exact same results on version 2022b. And let’s not even mention the toolboxes (if you’re not familiar with MATLAB’s toolboxes, they’re the equivalent of packages or libraries in other programming languages). These evolve as well, and there’s no guarantee that you can purchase older versions of said toolboxes. And it’s likely that newer versions of toolboxes cannot even run on older versions of Matlab.\nAnd let me be clear, what I’m describing here with MATLAB could also be said for any other proprietary programs still commonly (unfortunately) used in research and in statistics (like STATA, SAS or SPSS). And even if some, or even all, of the editors of these proprietary tools provide ways to buy and run older versions of their software, my point is that the fact that you have to rely on them for this is a barrier to reproducibility, and there is no guarantee they will provide the option to purchase older versions forever. Also, who guarantees that the editors of these tools will be around forever? Or, and that’s more likely, that they will keep offering a program that you install on your machine instead of shifting to a subscription based model?\nFor just $199 a month, you can execute your SAS (or whatever) scripts on the cloud! Worry about data confidentiality? No worries, data gets encrypted and stored safely on our secure servers! Run your analysis from anywhere and don’t worry about losing your work if your cat knocks over your coffee on your laptop! And if you purchase the pro licence, for an additional $100 a month, you can even execute your code in parallel!\nThink this is science fiction? Google “SAS cloud” to see SAS’s cloud based offering.\n\n\n1.4.2 There are hidden dependencies that can hinder the reproducibility of a project\nThen there’s another problem: let’s suppose you’ve written a nice, thoroughly tested and documented workflow, and made it available on Github (and let’s even assume that the data is available for people to freely download, and that the paper is open access). Or, if you’re working in the private sector, you did everything above as well, the only difference being that the workflow is only available to people inside the company instead of being available freely and publicly online.\nLet’s further assume that you’ve used R or Python, or any other open source programming language. Could this study/analysis be said to be reproducible? Well, if the analysis ran on a proprietary operating system, then the conclusion is: your project is not reproducible.\nThis is because the operating system the code runs on can also influence the outputs that your pipeline builds. There are some particularities in operating systems that may make certain things work differently. Admittedly, this is in practice rarely a problem, but it does happen1, especially if you’re working with very high precision floating point arithmetic like you would do in the financial sector for instance.\nThankfully, there is no need to change operating systems to deal with this issue, and we will learn how to use Docker to safeguard against this problem.\n\n\n1.4.3 The requirements of a RAP\nSo where does that leave us? Basically, for something to be truly reproducible, it has to respect the following bullet points:\n\nSource code must obviously be available and thoroughly tested and documented (which is why we will be using Git and Github);\nAll the dependencies must be easy to find and install (we are going to deal with this using dependency management tools);\nTo be written with an open source programming language (nocode tools like Excel are by default non-reproducible because they can’t be used non-interactively, and which is why we are going to use the Python programming language);\nThe project needs to be run on an open source operating system (thankfully, we can deal with this without having to install and learn to use a new operating system, thanks to Docker);\nData and the paper/report need obviously to be accessible as well, if not publicly as is the case for research, then within your company. This means that the concept of “scripts and/or data available upon request” belongs in the trash.\n\n\n\n\nA real sentence from a real paper published in THE LANCET Regional Health. How about make the data available and I won’t scratch your car, how’s that for a reasonable request?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#are-there-different-types-of-reproducibility",
    "href": "intro.html#are-there-different-types-of-reproducibility",
    "title": "1  Introduction",
    "section": "1.5 Are there different types of reproducibility?",
    "text": "1.5 Are there different types of reproducibility?\nLet’s take one step back: we live in the real world, and in the real world, there are some constraints that are outside of our control. These constraints can make it impossible to build a true RAP, so sometimes we need to settle for something that might not be a true RAP, but a second or even third best thing.\nIn what follows, let’s assume this: in the discussion below, code is tested and documented, so let’s only discuss the code running the pipeline itself.\nThe worst reproducible pipeline would be something that works, but only on your machine. This can be simply due to the fact that you hardcoded paths that only exist on your laptop. Anyone wanting to rerun the pipeline would need to change the paths. This is something that needs to be documented in a README which we assumed was the case, so there’s that. But maybe this pipeline only runs on your laptop because the computational environment that you’re using is hard to reproduce. Maybe you use software, even if it’s open source software, that is not easy to install (anyone that tried to install R packages on Linux that depend on the {rJava} package know what I’m talking about).\nSo a least worse pipeline would be one that could be run more easily on any similar machine to yours. This could be achieved by not using hardcoded absolute paths, and by providing instructions to set up the environment. For example, in the case of Python, this could be as simple as providing a requirements.txt file that lists the dependencies of the project, and which could be easily install using pip:\npip install -r requirements.txt\nDoing this ensures that others, or future you, will be able to install the required packages to reproduce a study. However, this is not enough, and I will be talking about pipenv to do this kind of thing instead of pip. In the next chapter I’ll explain why.\nYou should also ensure that people run the same analysis on the same version of Python that was used to program it. Just installing the right packages is not enough. The same code can produce different results on different versions of Python, or not even work at all. If you’ve been using Python for some time, you certainly remember the switch from Python 2 to Python 3… and who knows, the switch to Python 4 might be just as painful!\nThe take-away message is that counting on the language itself being stable through time as a sufficient condition for reproducibility is not enough. We have to set up the code in a way that it actually is reproducible and explicitely deal with versions of the language itself.\nSo what does this all mean? This means that reproducibility is on a continuum, and depending on the constraints you face your project can be “not very reproducible” to “totally reproducible”. Let’s consider the following list of anything that can influence how reproducible your project truly is:\n\nVersion of the programming language used;\nVersions of the packages/libraries of said programming language used;\nOperating System, and its version;\nVersions of the underlying system libraries (which often go hand in hand with OS version, but not necessarily).\nAnd even the hardware architecture that you run all that software stack on.\n\nSo by “reproducibility is on a continuum”, what I mean is that you could set up your project in a way that none, one, two, three, four or all of the preceding items are taken into consideration when making your project reproducible.\nThis is not a novel, or new idea. Peng (2011) already discussed this concept but named it the reproducibility spectrum. In part 2 of this book, I will reintroduce the idea and call it the “reproducibility iceberg”.\n\n\n\nThe reproducibility spectrum from Peng’s 2011 paper.\n\n\nLet me just finish this introduction by discussing the last item on the previous list: hardware architecture. You see, Apple has changed the hardware architecture of their computers recently. Their new computers don’t use Intel based hardware anymore, but instead Apple’s own proprietary architecture (Apple Silicon) based on the ARM specification. And what does that mean concretely? It means that all the binary packages that were built for Intel based Apple computers cannot run on their new computers (at least not without a compatibility layer). Which means that if you have a recent Apple Silicon Macbook and need to install old packages to rerun a project (and we will learn how to do this later in the book), these need to be compiled to work on Apple Silicon first. Now I have read about a compatibility layer called Rosetta which enables to run binaries compiled for the Intel architecture on the ARM architecture, and maybe this works well with older Python and package binaries compiled for Intel architecture. Maybe, I don’t know. But my point is that you never know what might come in the future, and thus needing to be able to compile from source is important, because compiling from source is what requires the least amount of dependencies that are outside of your control. Relying on binaries is not future-proof (and which is again, another reason why open-source tools are a hard requirement for reproducibility).\nAnd for you Windows users, don’t think that the preceding paragraph does not concern you. I think that it is very likely that Microsoft will push in the future for OEM manufacturers to build more ARM based computers. There is already an ARM version of Windows after all, and it has been around for quite some time, and I think that Microsoft will not kill that version any time in the future. This is because ARM is much more energy efficient than other architectures, and any manufacturer can build its own ARM cpus by purchasing a license, which can be quite interesting from a business perspective. For example in the case of Apple Silicon cpus, Apple can now get exactly the cpus they want for their machines and make their software work seamlessly with it (also, further locking in their users to their hardware). I doubt that others will pass the chance to do the same.\nAlso, something else that might happen is that we might move towards more and more cloud based computing, but I think that this scenario is less likely than the one from before. But who knows. And in that case it is quite likely that the actual code will be running on Linux servers that will likely be ARM based because of energy and licensing costs. Here again, if you want to run your historical code, you’ll have to compile old packages and R versions from source.\nOk, so this might seem all incredibly complicated. How on earth are we supposed to manage all these risks and balance the immediate need for results with the future need of rerunning an old project? And what if rerunning this old project is not even needed in the future?\nThis is where this book will help you. By employing the techniques discussed in this book, not only will it be very easy and quick to set up a project from the ground up that is truly reproducible, the very fact of building the project this way will also ensure that you avoid mistakes and producing results that are wrong. It will be easier and faster to iterate and improve your code, to collaborate, and ultimately to trust the results of your pipelines. So even if no one will rerun that code ever again, you will still benefit from the best practices presented in this book. Let’s dive in!\n\n\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–27.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "https://github.com/numpy/numpy/issues/9187↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "setting_up.html",
    "href": "setting_up.html",
    "title": "2  Setting up a development environment",
    "section": "",
    "text": "2.1 Why is installing Python such a hard problem?\nIf you google “how to install Python” you will find a surprising amount of articles explaining how to do it. I say “surprising amount” because one might expect to install Python like any other piece of software. If you’re already familiar with R, you could think that installing Python would be done the same way: download the installer for your operating system, and then install it. And, actually, you can do just that for Python as well. So why are there 100s of articles online explaining how to install Python, and why aren’t all of these articles simply telling you to download the installer to install Python? Why did I write this chapter on installing Python?\nWell, there are several thing that we need to deal with if we want to install and use Python the “right way”. First of all, Python is pre-installed on Linux distributions and older versions of macOS. So if you’re using one of these operating systems, you could use the built-in Python interpreter, but this is not recommended. The reason being that these bundled versions are generally older, and that you don’t control their upgrade process, as these get updated alongside the operating system. On Windows and newer versions of macOS, Python is, as far as I know, never bundled, so you’d need to install it anyways.\nAnother reason why you should install a Python version and manage it yourself, is that newer Python versions can introduce breaking changes, making code written for an earlier version of Python not run on a newer version of Python. This is not a Python-specific issue: it happens with any programming language. So this means that ideally you would want to bundle a Python version with your project’s code.\nThe same holds true for individual packages: newer versions of packages might not even work with older releases of Python, so to avoid any issues, an analysis would get bundled with a Python release and Python packages. This bundle is what I call a development environment, and in order to build such development environments, specific tools have to be used. And there’s a lot of these tools in the Python ecosystem… so much so that when you’re first starting, you might get lost. So here are the two tools that I use for this, and that I think work quite well together: micromamba and pipenv.\nThe workflow is as follows:\nIn the following sections I detail this process.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting up a development environment</span>"
    ]
  },
  {
    "objectID": "setting_up.html#why-is-installing-python-such-a-hard-problem",
    "href": "setting_up.html#why-is-installing-python-such-a-hard-problem",
    "title": "2  Setting up a development environment",
    "section": "",
    "text": "Install micromamba, a lightweight package manager.\nUsing micromamba, create an environment that contains a Python interpreter and the pipenv package.\nUsing this environment, install the required packages using pipenv.\npipenv will automatically generate two very useful files, Pipfile and Pipfile.lock.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting up a development environment</span>"
    ]
  },
  {
    "objectID": "setting_up.html#creating-a-project-specific-development-environment",
    "href": "setting_up.html#creating-a-project-specific-development-environment",
    "title": "2  Setting up a development environment",
    "section": "2.2 Creating a project-specific development environment",
    "text": "2.2 Creating a project-specific development environment\nWhat we want is to have project-specific development environments that should include a specific Python version, specific versions of Python packages and all the code to actually run the project. If you do this, you have already achieved a great deal to make your analysis reproducible. Some would even argue that it is enough!\nI’m going to assume that you don’t have any Python version available on your computer and need to get one. Let’s suppose that Python version 3.12.1 is the latest released version, and let’s also suppose that you would like to use that version to start working on a project. To first get the right Python interpreter ready, you should install micromamba. Please refer to the micromamba documentation here1 but on Linux, macOS and Git Bash on Windows (there’s also instructions for Powershell, if you don’t have Git Bash installed on Windows), it should be as easy as running this in your terminal:\n\"${SHELL}\" &lt;(curl -L micro.mamba.pm/install.sh)\nfor Poweshell use instead Invoke-Expression ((Invoke-WebRequest -Uri https://micro.mamba.pm/install.ps1).Content) instead.\nWe can now use micromamba to create an environment that will contain a Python interpreter for our project and pipenv. Type the following command to create this environment:\nmicromamba create -n housing python=3.12.1 pipenv\nThis will create an environment named “pipenv_env” that includes pipenv and the version 3.12.1 of Python. If you’re just starting a project, you can safely choose the very latest released version (check the releases pages on Python’s website).\nWe can now use pipenv to install the packages for our project. But why don’t we just use the more common pip instead of pipenv, or even micromamba which can also install any other Python package that we require for our projects? Why introduce yet another tool? In my opinion, pipenv has one absolutely crucial feature for reproducibility: pipenv enables deterministic builds, which means that when using pipenv, we will always get exactly the same packages installed.\n“But isn’t that exctaly what using requirements.txt file does?” you wonder. You are not entirely wrong. After all, if you make the effort to specify the packages you need, and their versions, wouldn’t running pip install -r requirements.txt also install exactly the same packages? (If you don’t know what a requirements.txt file is, you can think of it as a simple text file that lists the required packages and their versions for an analysis).\nWell, not quite. Imagine for example that you need a package called hello and you put it into your requirements.txt like so:\nhello==1.0.1\nSuppose that hello depends on another package called ciao. If you run pip install -r requirements.txt today, you’ll get hello at version 1.0.1 and ciao, say, at version 0.3.2. But if you run pip install -r requirements.txt in 6 months, you would still get hello at version 1.0.1 but you might get a newer version of ciao. This is because ciao itself is not specified in the requirements.txt, unless you made sure to add it (and then also add its dependencies, and their dependencies…). This mismatch in the versions of ciao can cause issues. pipenv takes care of this for you by generating a so-called lock file automatically, and adds further security checks by comparing sha256 hashes from the lock file to the ones from the downloaded packages, making sure that you are actually installing what you believe you are.\nWhat about using micromamba? micromamba could indeed be used to install the project’s dependencies, but would require another tool called conda-lock to generate lock files, and in my experience, using conda-lock doesn’t always work. I have had 0 issues with pipenv on the other hand.\nIn any case, the point is that you should use a tool that specifies dependencies very strictly and precisely. Use whatever you’re comfortable with if you already are familiar with one such tool. If not, and you want to follow along, use pipenv but take some time to check out other options. I personally use Nix, which is not specific to Python, but I decided not to discuss Nix in this book, because to properly discuss it, it would require a book on its own.\nNow that pipenv is installed, let’s start using it to install the packages we need for our project. Because the Python interpreter was installed using micromamba, we either need to activate the environment to get access to it, or we should use micromamba run to run the Python intpreter from this environment. First, create a folder called housing, which will contain our analysis scripts. Then, from that folder, run the following command:\nmicromamba run -n housing pipenv install polars==1.1.0 plotnine beautifulsoup4 pandas plotnine skimpy\nAs you can see, I chose to install specific versions of polars. This is because I want you to follow along with the same versions as in the book. You could remove the ==x.y.z string from the command above to install the latest versions of polars available if you prefer, but then there would be no guarantee that you would find the same results as I do in the remainder of the book. You could also specify versions for the other packages if you wish.\nYou should now see two new files in the housing folder, Pipfile and Pipfile.lock. Start by opening Pipfile, it should look like this:\n[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\npolars = \"==1.1.0\"\nplotnine = \"*\"\nbeautifulsoup4 = \"*\"\npandas = \"*\"\nskimpy = \"*\"\n\n[dev-packages]\n\n[requires]\npython_version = \"3.12\"\nI think that this file is pretty self-evident: the packages being used for this project are listed alongside their versions. The Python version is also listed. Sometimes, depending on how you set up the project, it could happen that the Python version listed is not the one you want for your project. In this case, I highly recommend you change the version to the right one. Also, you’ll notice that here the Python version is “3.12”, but we specified version “3.12.1” with micromamba when we created the environment. I would recommend that you add the missing “.1” for maximum reproducibility. If you edited the Pipfile then you need to run pipenv lock to regenerate the Pipfile.lock file as well:\nmicromamba run -n housing pipenv lock\nthis will make sure to also set the required/correct Python version in there.\nIf you open the Pipfile.lock in a text editor, you will see that it is a json file and that also lists the dependecies of your project, but also the dependencies’ dependencies. You will also notice several fields called hashes. These are there for security reasons: whenever someone, (or you in the future) will regenerate this environment, the packages will get downloaded and their hashes will get compared to the ones listed in the Pipfile.lock. If they don’t match, then something very wrong is happening and packages won’t get installed. These two files are very important, because they will make sure that it will be possible to regenerate the same environment on another machine.\nTo check whether everything installed correctly, drop into the development shell using:\nmicromamba run -r housing pipenv shell\nand check that the right version of Python is being used:\npython --version\nThis should print Python 3.12.1 in the terminal. Start the Python interpreter and let’s check polars’s version:\npython\nThen check that the correct versions of the packages were installed:\n\nimport polars as pl\npl.__version__\n\n'0.20.15'\n\n\nYou should see 1.1.0 as the listed version. Quit the shell, and then quit the environment with exit.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting up a development environment</span>"
    ]
  },
  {
    "objectID": "setting_up.html#one-last-thing",
    "href": "setting_up.html#one-last-thing",
    "title": "2  Setting up a development environment",
    "section": "2.3 One last thing",
    "text": "2.3 One last thing\nBefore continuing, it would be nice if we would automatically drop into the environment each time we are in the right folder; so for example, each time we navigate to the housing folder for our project on housing, the housing environment starts. We can achieve this by using a tool called direnv.\n\nInstall direnv\nTake note of the command when you run micromamba run -n housing pipenv shell in my case. /home/b-rodrigues/.local/share/virtualenvs/py_housing-MvX0wC5I/bin/activate`\nput that in a .envrc file in your folder\ndirenv allow\nprofit",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting up a development environment</span>"
    ]
  },
  {
    "objectID": "setting_up.html#a-high-level-description-of-how-to-set-up-a-project",
    "href": "setting_up.html#a-high-level-description-of-how-to-set-up-a-project",
    "title": "2  Setting up a development environment",
    "section": "2.4 A high-level description of how to set up a project",
    "text": "2.4 A high-level description of how to set up a project\nOk, so to summarise, we install micromamba which will make it easy to install any version of Python that we require, and we also installed pipenv, which we use to install packages. The advantage of using pipenv is that we get deterministic builds, and pipenv works well with micromamba to build project-specific environments.\nThe way I would suggest you use these tools now, is that for each project, you install the latest available version of Python and then install packages by specyifing their versions, like so:\nmicromamba create -n project_name python=X.YY.Z pipenv \nthen, use this environment in a fresh folder to install the packages you need:\nmicromamba run -n housing pipenv install beautifulsoup4==4.12.2 pandas==2.1.4 plotnine==0.12.4 skimpy==0.0.11\n(Replace the versions of Python and packages by the latest, or those you need.)\nThis will ensure that your project uses the correct software stack, and that collaborators or future you will be able to regenerate this environment by calling pipenv sync. pipenv sync differs from pipenv install because it will not touch the Pipfile.lock file, and will just download the packages as listed in the lock file. This is also the command that we will use later, in the chapter on CI/CD.\nIf you need to add packages to an environment, run: micromamba run -n housing pipenv install great_tables (or simply if pipenv install great_tables if you’re already in the activated housing environment).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting up a development environment</span>"
    ]
  },
  {
    "objectID": "setting_up.html#footnotes",
    "href": "setting_up.html#footnotes",
    "title": "2  Setting up a development environment",
    "section": "",
    "text": "https://mamba.readthedocs.io/en/latest/installation/micromamba-installation.html#automatic-install↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting up a development environment</span>"
    ]
  },
  {
    "objectID": "project_start.html",
    "href": "project_start.html",
    "title": "3  Project start",
    "section": "",
    "text": "3.1 The Polars package and why you should ditch Pandas in its favour",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Project start</span>"
    ]
  },
  {
    "objectID": "project_start.html#housing-in-luxembourg",
    "href": "project_start.html#housing-in-luxembourg",
    "title": "3  Project start",
    "section": "3.2 Housing in Luxembourg",
    "text": "3.2 Housing in Luxembourg\nWe are going to download data about house prices in Luxembourg. Luxembourg is a little Western European country the author hails from that looks like a shoe and is about the size of .98 Rhode Islands. Did you know that Luxembourg is a constitutional monarchy, and not a kingdom like Belgium, but a Grand-Duchy, and actually the last Grand-Duchy in the World? Also, what you should know to understand what we will be doing is that the country of Luxembourg is divided into Cantons, and each Cantons into Communes. If Luxembourg was the USA, Cantons would be States and Communes would be Counties (or Parishes or Boroughs). What’s confusing is that “Luxembourg” is also the name of a Canton, and of a Commune, which also has the status of a city and is the capital of the country. So Luxembourg the country, is divided into Cantons, one of which is called Luxembourg as well, cantons are divided into communes, and inside the canton of Luxembourg, there’s the commune of Luxembourg which is also the city of Luxembourg, sometimes called Luxembourg City, which is the capital of the country.\n\n\n\nLuxembourg is about as big as the US State of Rhode Island.\n\n\nWhat you should also know is that the population is about 645,000 as of writing (January 2023), half of which are foreigners. Around 400,000 persons work in Luxembourg, of which half do not live in Luxembourg; so every morning from Monday to Friday, 200,000 people enter the country to work and then leave in the evening to go back to either Belgium, France or Germany, the neighbouring countries. As you can imagine, this puts enormous pressure on the transportation system and on the roads, but also on the housing market; everyone wants to live in Luxembourg to avoid the horrible daily commute, and everyone wants to live either in the capital city, or in the second largest urban area in the south, in a city called Esch-sur-Alzette.\nThe plot below shows the value of the House Price Index over time for Luxembourg and the European Union:\n\n\n\n\n\n\n\n\n\nIf you want to download the data, click here1.\nLet us paste the definition of the HPI in here (taken from the HPI’s metadata2 page):\nThe House Price Index (HPI) measures inflation in the residential property market. The HPI captures price changes of all types of dwellings purchased by households (flats, detached houses, terraced houses, etc.). Only transacted dwellings are considered, self-build dwellings are excluded. The land component of the dwelling is included.\nSo from the plot, we can see that the price of dwellings more than doubled between 2010 and 2021; the value of the index is 214.81 in 2021 for Luxembourg, and 138.92 for the European Union as a whole.\nThere is a lot of heterogeneity though; the capital and the communes right next to the capital are much more expensive than communes from the less densely populated north, for example. The south of the country is also more expensive than the north, but not as much as the capital and surrounding communes. Not only is price driven by demand, but also by scarcity; in 2021, 0.5% of residents owned 50% of the buildable land for housing purposes (Source: Observatoire de l’Habitat, Note 29, archived download link3).\nOur project will be quite simple; we are going to download some data, supplied as an Excel file, compiled by the Housing Observatory (Observatoire de l’Habitat, a service from the Ministry of Housing, which monitors the evolution of prices in the housing market, among other useful services like the identification of vacant lots). The advantage of their data when compared to Eurostat’s data is that the data is disaggregated by commune. The disadvantage is that they only supply nominal prices, and no index (and the data is trapped inside Excel and not ready for analysis with R). Nominal prices are the prices that you read on price tags in shops. The problem with nominal prices is that it is difficult to compare them through time. Ask yourself the following question: would you prefer to have had 500€ (or USDs) in 2003 or in 2023? You probably would have preferred them in 2003, as you could purchase a lot more with $500 then than now. In fact, according to a random inflation calculator I googled, to match the purchasing power of $500 in 2003, you’d need to have $793 in 2023 (and I’d say that we find very similar values for €). But it doesn’t really matter if that calculation is 100% correct: what matters is that the value of money changes, and comparisons through time are difficult, hence why an index is quite useful. So we are going to convert these nominal prices to real prices. Real prices take inflation into account and so allow us to compare prices through time.\nSo to summarise; our goal is to:\n\nGet data trapped inside an Excel file into a neat data frame;\nConvert nominal to real prices using a simple method;\nMake some tables and plots and call it a day (for now).\n\nWe are going to start in the most basic way possible; we are simply going to write a script and deal with each step separately.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Project start</span>"
    ]
  },
  {
    "objectID": "project_start.html#saving-trapped-data-from-excel",
    "href": "project_start.html#saving-trapped-data-from-excel",
    "title": "3  Project start",
    "section": "3.3 Saving trapped data from Excel",
    "text": "3.3 Saving trapped data from Excel\nGetting data from Excel into a tidy data frame can be very tricky. This is because very often, Excel is used as some kind of dashboard or presentation tool. So data is made human-readable, in contrast to machine-readable. Let us quickly discuss this topic as it is essential to grasp the difference between the two (and in our experience, a lot of collective pain inflicted to statisticians and researchers could have been avoided if this concept was more well-known). The picture below shows an Excel file made for human consumption:\n\n\n\nAn Excel file meant for human eyes.\n\n\nSo why is this file not machine-readable? Here are some issues:\n\nThe table does not start in the top-left corner of the spreadsheet, which is where most importing tools expect it to be;\nThe spreadsheet starts with a header that contains an image and some text;\nNumbers are text and use “,” as the thousands separator;\nYou don’t see it in the screenshot, but each year is in a separate sheet.\n\nThat being said, this Excel file is still very tame, and going from this Excel to a tidy data frame will not be too difficult. In fact, we suspect that whoever made this Excel file is well aware of the contradicting requirements of human and machine-readable formatting of data, and strove to find a compromise. Because more often than not, getting human-readable data into a machine-readable format is a nightmare. We could call data like this machine-friendly data.\nIf you want to follow along, you can download the Excel file here4 (downloaded on January 2023 from the luxembourguish open data portal5). But you don’t need to follow along with code, because I will link the completed scripts for you to download later.\nEach sheet contains a dataset with the following columns:\n\nCommune: the commune (the smallest administrative division of territory);\nNombre d’offres: the total number of selling offers;\nPrix moyen annoncé en Euros courants: Average selling price in nominal Euros;\nPrix moyen annoncé au m2 en Euros courants: Average selling price in square meters in nominal Euros.\n\nFor ease of presentation, I’m going to show you each step of the analysis here separately, but I’ll be putting everything together in a single script once I’m done explaining each step. So first, let’s load some packages:\n\nimport polars as pl\nimport polars.selectors as cs\nimport re\n\nI will be using the polars package to manipulate data.\nNext, the code below downloads the data, and puts it in a data frame:\n\n# The url below points to an Excel file\n# hosted on the book’s github repository\nurl &lt;- \"https://is.gd/1vvBAc\"\n\nraw_data &lt;- tempfile(fileext = \".xlsx\")\n\ndownload.file(url, raw_data,\n              method = \"auto\",\n              mode = \"wb\")\n\nsheets &lt;- excel_sheets(raw_data)\n\nread_clean &lt;- function(..., sheet){\n  read_excel(..., sheet = sheet) |&gt;\n    mutate(year = sheet)\n}\n\nraw_data &lt;- map(\n  sheets,\n  ~read_clean(raw_data,\n              skip = 10,\n              sheet = .)\n                   ) |&gt;\n  bind_rows() |&gt;\n  clean_names()\n\nraw_data &lt;- raw_data |&gt;\n  rename(\n    locality = commune,\n    n_offers = nombre_doffres,\n    average_price_nominal_euros = prix_moyen_annonce_en_courant,\n    average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant,\n    average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant\n  ) |&gt;\n  mutate(locality = str_trim(locality)) |&gt;\n  select(year, locality, n_offers, starts_with(\"average\"))\n\nIf you are familiar with the {tidyverse} (Wickham et al. 2019) the above code should be quite easy to follow. We start by downloading the raw Excel file and saving the sheet names into a variable. We then use a function called read_clean(), which takes the path to the Excel file and the sheet names as an argument to read the required sheet into a data frame. We use skip = 10 to skip the first 10 lines in each Excel sheet because the first 10 lines contain a header. The last thing this function does is add a new column called year which contains the year of the data. We’re lucky because the sheet names are the years: “2010”, “2011” and so on. We then map this function to the list of sheet names, thus reading in all the data from all the sheets into one list of data frames. We then use bind_rows(), to bind each data frame into a single data frame, by row. Finally, we rename the columns (by translating their names from French to English) and only select the required columns. If you don’t understand each step of what is going on, don’t worry too much about it; this book is not about learning how to use R.\nRunning this code results in a neat data set:\n\nraw_data\n\nBut there’s a problem: columns that should be of type numeric are of type character instead (average_price_nominal_euros and average_price_m2_nominal_euros). There’s also another issue, which you would eventually catch as you’ll explore the data: the naming of the communes is not consistent. Let’s take a look:\n\nraw_data |&gt;\n  filter(grepl(\"Luxembourg\", locality)) |&gt;\n  count(locality)\n\nWe can see that the city of Luxembourg is spelled in two different ways. It’s the same with another commune, Pétange:\n\nraw_data |&gt;\n  filter(grepl(\"P.tange\", locality)) |&gt;\n  count(locality)\n\nSo sometimes it is spelled correctly, with an “é”, sometimes not. Let’s write some code to correct both these issues:\n\nraw_data &lt;- raw_data |&gt;\n  mutate(\n    locality = ifelse(grepl(\"Luxembourg-Ville\", locality),\n                      \"Luxembourg\",\n                      locality),\n         locality = ifelse(grepl(\"P.tange\", locality),\n                           \"Pétange\",\n                           locality)\n         ) |&gt;\n  mutate(across(starts_with(\"average\"),\n         as.numeric))\n\nNow this is interesting – converting the average columns to numeric resulted in some NA values. Let’s see what happened:\n\nraw_data |&gt;\n  filter(is.na(average_price_nominal_euros))\n\nIt turns out that there are no prices for certain communes, but that we also have some rows with garbage in there. Let’s go back to the raw data to see what this is about:\n\n\n\nAlways look at your data.\n\n\nSo it turns out that there are some rows that we need to remove. We can start by removing rows where locality is missing. Then we have a row where locality is equal to “Total d’offres”. This is simply the total of every offer from every commune. We could keep that in a separate data frame, or even remove it. The very last row states the source of the data and we can also remove it. Finally, in the screenshot above, we see another row that we don’t see in our filtered data frame: one where n_offers is missing. This row gives the national average for columns average_prince_nominal_euros and average_price_m2_nominal_euros. What we are going to do is create two datasets: one with data on communes, and the other on national prices. Let’s first remove the rows stating the sources:\n\nraw_data &lt;- raw_data |&gt;\n  filter(!grepl(\"Source\", locality))\n\nLet’s now only keep the communes in our data:\n\ncommune_level_data &lt;- raw_data |&gt;\n    filter(!grepl(\"nationale|offres\", locality),\n           !is.na(locality))\n\nAnd let’s create a dataset with the national data as well:\n\ncountry_level &lt;- raw_data |&gt;\n  filter(grepl(\"nationale\", locality)) |&gt;\n  select(-n_offers)\n\noffers_country &lt;- raw_data |&gt;\n  filter(grepl(\"Total d.offres\", locality)) |&gt;\n  select(year, n_offers)\n\ncountry_level_data &lt;- full_join(country_level, offers_country) |&gt;\n  select(year, locality, n_offers, everything()) |&gt;\n  mutate(locality = \"Grand-Duchy of Luxembourg\")\n\nNow the data looks clean, and we can start the actual analysis… or can we? Before proceeding, it would be nice to make sure that we got every commune in there. For this, we need a list of communes from Luxembourg. Thankfully, Wikipedia has such a list6.\nAn issue with scraping tables off the web is that they might change in the future. It is therefore a good idea to save the page by right clicking on it and then selecting save as, and then re-hosting it. I use Github pages to re-host the Wikipedia page above here7. I now have full control of this page, and won’t get any bad surprises if someone decides to eventually update it. Instead of re-hosting it, you could simply save it as any other file of your project.\nSo let’s scrape and save this list:\n\ncurrent_communes &lt;- \"https://is.gd/lux_communes\" |&gt;\n  rvest::read_html() |&gt;\n  rvest::html_table() |&gt;\n  purrr::pluck(2) |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::filter(name_2 != \"Name\") |&gt;\n  dplyr::rename(commune = name_2) |&gt;\n  dplyr::mutate(commune = stringr::str_remove(commune, \" .$\"))\n\nWe scrape the table from the re-hosted Wikipedia page using {rvest}. rvest::html_table() returns a list of tables from the Wikipedia table, and then we use purrr::pluck() to keep the second table from the website, which is what we need (I made the calls to the packages explicit, because you might not be familiar with these packages). janitor::clean_names() transforms column names written for human eyes into machine-friendly names (for example Growth rate in % would be transformed to growth_rate_in_percent) and then I use the {dplyr} package for some further cleaning and renaming; the very last step removes a dagger symbol next to certain communes names, in other words it turns “Commune †” into “Commune”.\nLet’s see if we have all the communes in our data:\n\nsetdiff(unique(commune_level_data$locality),\n        current_communes$commune)\n\nWe see many communes that are in our commune_level_data, but not in current_communes. There’s one obvious reason: differences in spelling, for example, “Kaerjeng” in our data, but “Käerjeng” in the table from Wikipedia. But there’s also a less obvious reason; since 2010, several communes have merged into new ones. So there are communes that are in our data in 2010 and 2011, but disappear from 2012 onwards. So we need to do several things: first, get a list of all existing communes from 2010 onwards, and then, harmonise spelling. Here again, we can use a list from Wikipedia, and here again, I decide to re-host it on Github pages to avoid problems in the future:\n\nformer_communes &lt;- \"https://is.gd/lux_former_communes\" |&gt;\n  rvest::read_html() |&gt;\n  rvest::html_table() |&gt;\n  purrr::pluck(3) |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::filter(year_dissolved &gt; 2009)\n\nformer_communes\n\nAs you can see, since 2010 many communes have merged to form new ones. We can now combine the list of current and former communes, as well as harmonise their names:\n\ncommunes &lt;- unique(c(former_communes$name,\n                     current_communes$commune))\n# we need to rename some communes\n\n# Different spelling of these communes between wikipedia and the data\n\ncommunes[which(communes == \"Clemency\")] &lt;- \"Clémency\"\ncommunes[which(communes == \"Redange\")] &lt;- \"Redange-sur-Attert\"\ncommunes[which(communes == \"Erpeldange-sur-Sûre\")] &lt;- \"Erpeldange\"\ncommunes[which(communes == \"Luxembourg City\")] &lt;- \"Luxembourg\"\ncommunes[which(communes == \"Käerjeng\")] &lt;- \"Kaerjeng\"\ncommunes[which(communes == \"Petange\")] &lt;- \"Pétange\"\n\nLet’s run our test again:\n\nsetdiff(unique(commune_level_data$locality),\n        communes)\n\nGreat! When we compare the communes that are in our data with every commune that has existed since 2010, we don’t have any commune that is unaccounted for. So are we done with cleaning the data? Yes, we can now start with analysing the data. Take a look here8 to see the finalised script. Also read some of the comments that I’ve added. This is a typical R script, and at first glance, one might wonder what is wrong with it. Actually, not much, but the problem if you leave this script as it is, is that it is very likely that we will have problems rerunning it in the future. As it turns out, this script is not reproducible. But we will discuss this in much more detail later on. For now, let’s analyse our cleaned data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Project start</span>"
    ]
  },
  {
    "objectID": "project_start.html#analysing-the-data",
    "href": "project_start.html#analysing-the-data",
    "title": "3  Project start",
    "section": "3.4 Analysing the data",
    "text": "3.4 Analysing the data\nWe are now going to analyse the data. The first thing we are going to do is compute a Laspeyeres price index. This price index allows us to make comparisons through time; for example, the index at year 2012 measures how much more expensive (or cheaper) housing became relative to the base year (2010). However, since we only have one ‘good’ (housing), this index becomes quite simple to compute: it is nothing but the prices at year t divided by the prices in 2010 (if we had a basket of goods, we would need to use the Laspeyeres index formula to compute the index at all periods).\nFor this section, I will perform a rather simple analysis. I will immediately show you the R script: take a look at it here9. For the analysis I selected 5 communes and plotted the evolution of prices compared to the national average.\nThis analysis might seem trivially simple, but it contains all the needed ingredients to illustrate everything else that I’m going to teach you in this book.\nMost analyses would stop here: after all, we have what we need; our goal was to get the plots for the 5 communes of Luxembourg, Esch-sur-Alzette, Mamer, Schengen (which gave its name to the Schengen Area10) and Wincrange. However, let’s ask ourselves the following important questions:\n\nHow easy would it be for someone else to rerun the analysis?\nHow easy would it be to update the analysis once new data gets published?\nHow easy would it be to reuse this code for other projects?\nWhat guarantee do we have that if the scripts get run in 5 years, with the same input data, we get the same output?\n\nLet’s answer these questions one by one.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Project start</span>"
    ]
  },
  {
    "objectID": "project_start.html#your-project-is-not-done",
    "href": "project_start.html#your-project-is-not-done",
    "title": "3  Project start",
    "section": "3.5 Your project is not done",
    "text": "3.5 Your project is not done\n\n3.5.1 How easy would it be for someone else to rerun the analysis?\nThe analysis is composed of two R scripts, one to prepare the data, and another to actually run the analysis proper. Performing the analysis might seem quite easy, because each script contains comments as to what is going on, and the code is not that complicated. However, we are missing any project-level documentation that would provide clear instructions as to how to run the analysis. This might seem simple for us who wrote these scripts, but we are familiar with R, and this is still fresh in our brains. Should someone less familiar with R have to run the script, there is no clue for them as to how they should do it. And of course, should the analysis be more complex (suppose it’s composed of dozens of scripts), this gets even worse. It might not even be easy for you to remember how to run this in 5 months!\nAnd what about the required dependencies? Many packages were used in the analysis. How should these get installed? Ideally, the same versions of the packages you used and the same version of R should get used by that person to rerun the analysis.\nAll of this still needs to be documented, but listing the packages that were used for an analysis and their versions takes quite some time. Thankfully, in part 2, we will learn about the {renv} package to deal with this in a couple lines of code.\n\n\n3.5.2 How easy would it be to update the project?\nIf new data gets published, all the points discussed previously are still valid, plus you need to make sure that the updated data is still close enough to the previous data such that it can pass through the data cleaning steps you wrote. You should also make sure that the update did not introduce a mistake in past data, or at least alert you if that is the case. Sometimes, when new years get added, data for previous years also get corrected, so it would be nice to make sure that you know this. Also, in the specific case of our data, communes might get fused into a new one, or maybe even divided into smaller communes (even though this has not happened in a long time, it is not entirely out of the question).\nIn summary, what is missing from the current project are enough tests to make sure that an update to the data can happen smoothly.\n\n\n3.5.3 How easy would it be to reuse this code for another project?\nSaid plainly, not very easy. With code in this state you have no choice but to copy and paste it into a new script and change it adequately. For re-usability, nothing beats structuring your code into functions and ideally you would even package them. We are going to learn just that in future chapters of this book.\nBut sometimes you might not be interested in reusing code for another project: however, even if that’s the case, structuring your code into functions and packaging them makes it easy to reuse code even inside the same project. Look at the last part of the analysis.R script: we copied and pasted the same code 5 times and only slightly changed it. We are going to learn how not to repeat ourselves by using functions and you will immediately see the benefits of writing functions, even when simply reusing them inside the same project.\n\n\n3.5.4 What guarantee do we have that the output is stable through time?\nNow this might seem weird: after all, if we start from the same dataset, does it matter when we run the scripts? We should be getting the same result if we build the project today, in 5 months or in 5 years. Well, not necessarily. While it is true that R is quite stable, this cannot necessarily be said of the packages that we use. There is no guarantee that the authors of the packages will not change the package’s functions to work differently, or take arguments in a different order, or even that the packages will all be available at all in 5 years. And even if the packages are still available and function the same, bugs in the packages might get corrected which could alter the result. This might seem like a non-problem; after all, if bugs get corrected, shouldn’t you be happy to update your results as well? But this depends on what it is we’re talking about. Sometimes it is necessary to reproduce results exactly as they were, even if they were wrong, for example in the context of an audit.\nSo we also need a way to somehow snapshot and freeze the computational environment that was used to create the project originally.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Project start</span>"
    ]
  },
  {
    "objectID": "project_start.html#conclusion",
    "href": "project_start.html#conclusion",
    "title": "3  Project start",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nWe now have a basic analysis that has all we need to get started. In the coming chapters, we are going to learn about topics that will make it easy to write code that is more robust, better documented and tested, and most importantly easy to rerun (and thus to reproduce the results). The first step will actually not involve having to start rewriting our scripts though; next, we are going to learn about Git, a tool that will make our life easier by versioning our code.\n\n\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Project start</span>"
    ]
  },
  {
    "objectID": "project_start.html#footnotes",
    "href": "project_start.html#footnotes",
    "title": "3  Project start",
    "section": "",
    "text": "https://is.gd/AET0ir↩︎\nhttps://archive.is/OrQwA, archived link for posterity.↩︎\nhttps://archive.org/download/note-29/note-29.pdf↩︎\nhttps://is.gd/1vvBAc↩︎\nhttps://data.public.lu/en/datasets/prix-annonces-des-logements-par-commune/↩︎\nhttps://w.wiki/6nPu↩︎\nhttps://is.gd/lux_communes↩︎\nhttps://is.gd/7PhUjd↩︎\nhttps://is.gd/qCJEbi↩︎\nhttps://en.wikipedia.org/wiki/Schengen_Area↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Project start</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Peng, Roger D. 2011. “Reproducible Research in Computational\nScience.” Science 334 (6060): 1226–27.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686.",
    "crumbs": [
      "References"
    ]
  }
]